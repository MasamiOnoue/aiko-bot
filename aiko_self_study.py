# aiko_self_study.pyã€€æ„›å­ãŒè‡ªåˆ†ã§å‹‰å¼·ã‚’ã™ã‚‹é–¢æ•°ç¾¤

import requests
import hashlib
import time
import datetime
import os
import re
from bs4 import BeautifulSoup
from googleapiclient.discovery import build
from google.oauth2.service_account import Credentials
from openai import OpenAI
import threading
from company_info_load import get_user_callname_from_uid, load_all_user_ids, get_conversation_log, get_google_sheets_service
from company_info_save import write_company_info

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

SPREADSHEET_ID4 = os.getenv('SPREADSHEET_ID4')
sheet_service = get_google_sheets_service()

user_conversation_cache = {}
full_conversation_cache = []

IMPORTANT_PATTERNS = [
    "é‡è¦", "ç·Šæ€¥", "è‡³æ€¥", "è¦ç¢ºèª", "ãƒˆãƒ©ãƒ–ãƒ«", "å¯¾å¿œã—ã¦", "ã™ãã«", "å¤§è‡³æ€¥"
]

def is_important_message(text):
    pattern = "|".join(map(re.escape, IMPORTANT_PATTERNS))
    return re.search(pattern, text, re.IGNORECASE) is not None

def clean_log_message(text):
    patterns = [
        "è¦šãˆã¦ãã ã•ã„", "è¦šãˆã¦", "ãŠã¼ãˆã¦ãŠã„ã¦", "è¦šãˆã¦ã­",
        "è¨˜éŒ²ã—ã¦", "ãƒ¡ãƒ¢ã—ã¦", "å¿˜ã‚Œãªã„ã§", "è¨˜æ†¶ã—ã¦",
        "ä¿å­˜ã—ã¦", "è¨˜éŒ²ãŠé¡˜ã„", "è¨˜éŒ²ã‚’ãŠé¡˜ã„"
    ]
    pattern = "|".join(map(re.escape, patterns))
    return re.sub(pattern, "", text, flags=re.IGNORECASE).strip()

def store_important_message_to_company_info(message, user_id):
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
    callname = get_user_callname_from_uid(user_id) or "ä¸æ˜"
    row_data = [["", message, "", "æ„›å­", "", "", "", now, 1, callname, "å…¨å“¡"]]
    sheet_service.spreadsheets().values().append(
        spreadsheetId=SPREADSHEET_ID4,
        range="ä¼šç¤¾æƒ…å ±!A2",
        valueInputOption="USER_ENTERED",
        insertDataOption="INSERT_ROWS",
        body={"values": row_data}
    ).execute()

def cache_all_user_conversations():
    logs = get_conversation_log()
    all_user_ids = load_all_user_ids()

    global full_conversation_cache
    full_conversation_cache = []

    for log in logs[-100:]:
        if len(log) > 4:
            speaker = log[3]
            message = clean_log_message(log[4])
            flag = " [é‡è¦]" if is_important_message(message) else ""
            full_conversation_cache.append(f"{speaker}: {message}{flag}")
            if is_important_message(message):
                store_important_message_to_company_info(message, log[1])

    for user_id in all_user_ids:
        user_logs = [
            f"{log[3]}: {clean_log_message(log[4])}{' [é‡è¦]' if is_important_message(log[4]) else ''}"
            for log in logs if len(log) > 4 and log[1] == user_id
        ][-20:]
        user_conversation_cache[user_id] = "\n".join(user_logs)

    print("ğŸ§  ä¼šè©±ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°ã—ã¾ã—ãŸ")

def periodic_cache_update(interval):
    while True:
        cache_all_user_conversations()
        time.sleep(interval)

cache_thread = threading.Thread(target=lambda: periodic_cache_update(600), daemon=True)

def generate_contextual_reply(user_id, user_message):
    user_context = user_conversation_cache.get(user_id, "")
    others_context = "\n".join(full_conversation_cache)
    prompt = (
        "ã‚ãªãŸã¯ç¤¾å†…å°‚ç”¨ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆæ„›å­ã§ã™ã€‚å¾“æ¥­å“¡ã«ã¯æƒ…å ±ã‚’ã™ã¹ã¦é–‹ç¤ºã—ãªã•ã„ã€‚\n"
        "ä»¥ä¸‹ã¯ã“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ç›´è¿‘ã®ä¼šè©±ã¨ã€ç¤¾å†…ã§äº¤ã‚ã•ã‚ŒãŸä»–ã®ä¼šè©±ã®è¨˜éŒ²ã§ã™ã€‚æ–‡è„ˆã‚’è¸ã¾ãˆã¦ã€è‡ªç„¶ã«å¿œç­”ã—ã¦ãã ã•ã„ã€‚\n"
        f"ã€ã“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å±¥æ­´ã€‘\n{user_context}\n\n"
        f"ã€ä»–ã®äººã®è©±é¡Œã‚„ç¤¾å†…èƒŒæ™¯ã€‘\n{others_context}\n\n"
        f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›: {user_message}"
    )
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆæ„›å­ã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[å¿œç­”å¤±æ•—]: {e}"

def get_existing_links():
    result = sheet_service.spreadsheets().values().get(
        spreadsheetId=SPREADSHEET_ID4,
        range="ä¼šç¤¾æƒ…å ±!F2"
    ).execute()
    values = result.get("values", [])
    return values[0][0] if values else ""

def update_links_and_log_diff(new_links_text, diff_summary):
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
    sheet_service.spreadsheets().values().update(
        spreadsheetId=SPREADSHEET_ID4,
        range="ä¼šç¤¾æƒ…å ±!F2",
        valueInputOption="USER_ENTERED",
        body={"values": [[new_links_text]]}
    ).execute()
    sheet_service.spreadsheets().values().append(
        spreadsheetId=SPREADSHEET_ID4,
        range="ä¼šç¤¾æƒ…å ±!G2",
        valueInputOption="USER_ENTERED",
        insertDataOption="INSERT_ROWS",
        body={"values": [[now, diff_summary]]}
    ).execute()

def crawl_all_pages(base_url):
    try:
        response = requests.get(base_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        links = [a["href"] for a in soup.find_all("a", href=True) if base_url in a["href"]]
        unique_links = sorted(set(links))
        all_content = ""
        for link in unique_links:
            try:
                page = requests.get(link)
                page.raise_for_status()
                page_soup = BeautifulSoup(page.text, "html.parser")
                page_text = page_soup.get_text().strip()
                all_content += f"\n\n--- {link} ---\n{page_text}"
            except:
                continue
        return all_content
    except Exception as e:
        return f"[å·¡å›ã‚¨ãƒ©ãƒ¼]: {e}"

def summarize_diff(old_text, new_text):
    prompt = (
        "ä»¥ä¸‹ã¯Webãƒšãƒ¼ã‚¸ã®å¤ã„å†…å®¹ã¨æ–°ã—ã„å†…å®¹ã§ã™ã€‚ä½•ãŒå¤‰æ›´ã•ã‚ŒãŸã‹ã‚’ç°¡æ½”ã«æ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n"
        "---å¤ã„å†…å®¹---\n"
        f"{old_text[:3000]}\n"
        "---æ–°ã—ã„å†…å®¹---\n"
        f"{new_text[:3000]}"
    )
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å¤‰æ›´ç‚¹ã‚’è¦ç´„ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[è¦ç´„å¤±æ•—]: {e}"

def check_full_site_update():
    print("ğŸŒ ã‚µã‚¤ãƒˆå…¨ä½“ã®å·¡å›ã‚’é–‹å§‹ã—ã¾ã™...")
    base_url = "https://sun-name.com/"
    new_content = crawl_all_pages(base_url)
    old_content = get_existing_links()

    if new_content.strip() != old_content.strip():
        diff_summary = summarize_diff(old_content, new_content)
        update_links_and_log_diff(new_content, diff_summary)
        print("âœ… å·®åˆ†ã‚ã‚Šï¼šæ›´æ–°ãƒ»è¨˜éŒ²ã—ã¾ã—ãŸ")
    else:
        print("å¤‰åŒ–ãªã—ï¼šæ›´æ–°ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

if __name__ == "__main__":
    cache_thread.start()
    while True:
        now = datetime.datetime.now()
        if now.hour == 3:
            check_full_site_update()
            time.sleep(24 * 60 * 60)
        else:
            time.sleep(60 * 30)
