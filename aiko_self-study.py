# aiko_self_study.py

import requests
import hashlib
import time
import datetime
import os
from bs4 import BeautifulSoup
from googleapiclient.discovery import build
from google.oauth2.service_account import Credentials
import openai

# OpenAI APIã‚­ãƒ¼ï¼ˆRenderã®Environmentã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ï¼‰
openai.api_key = os.getenv("OPENAI_API_KEY")

# Spreadsheet IDï¼ˆä¼šç¤¾æƒ…å ±ï¼‰
SPREADSHEET_ID4 = os.getenv('SPREADSHEET_ID4')  # ä¼šç¤¾æƒ…å ±

# ä½¿ç”¨ã™ã‚‹ã‚¹ã‚³ãƒ¼ãƒ—ã¨èªè¨¼æƒ…å ±
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]
creds = Credentials.from_service_account_file("service_account.json", scopes=SCOPES)
sheet_service = build("sheets", "v4", credentials=creds)

# è£œè¶³æƒ…å ±åˆ—ã®å–å¾—ã¨æ›¸ãè¾¼ã¿
def get_existing_links():
    result = sheet_service.spreadsheets().values().get(
        spreadsheetId=SPREADSHEET_ID4,
        range="ä¼šç¤¾æƒ…å ±!F2"
    ).execute()
    values = result.get("values", [])
    return values[0][0] if values else ""

def update_links_and_log_diff(new_links_text, diff_summary):
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
    # è£œè¶³æƒ…å ±ï¼ˆFåˆ—ï¼‰æ›´æ–°
    sheet_service.spreadsheets().values().update(
        spreadsheetId=SPREADSHEET_ID4,
        range="ä¼šç¤¾æƒ…å ±!F2",
        valueInputOption="USER_ENTERED",
        body={"values": [[new_links_text]]}
    ).execute()
    # å·®åˆ†å±¥æ­´ï¼ˆGåˆ—ä»¥é™ï¼‰ã«è¿½è¨˜
    sheet_service.spreadsheets().values().append(
        spreadsheetId=SPREADSHEET_ID4,
        range="ä¼šç¤¾æƒ…å ±!G2",
        valueInputOption="USER_ENTERED",
        insertDataOption="INSERT_ROWS",
        body={"values": [[now, diff_summary]]}
    ).execute()

# ã‚µã‚¤ãƒˆå…¨ä½“ã‹ã‚‰ãƒªãƒ³ã‚¯ã¨ä¸­èº«ã‚’å–å¾—
def crawl_all_pages(base_url):
    try:
        response = requests.get(base_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        links = [a["href"] for a in soup.find_all("a", href=True) if base_url in a["href"]]
        unique_links = sorted(set(links))
        all_content = ""
        for link in unique_links:
            try:
                page = requests.get(link)
                page.raise_for_status()
                page_soup = BeautifulSoup(page.text, "html.parser")
                page_text = page_soup.get_text().strip()
                all_content += f"\n\n--- {link} ---\n{page_text}"
            except:
                continue
        return all_content
    except Exception as e:
        return f"[å·¡å›ã‚¨ãƒ©ãƒ¼]: {e}"

# OpenAIã§å·®åˆ†è¦ç´„
def summarize_diff(old_text, new_text):
    prompt = (
        "ä»¥ä¸‹ã¯Webãƒšãƒ¼ã‚¸ã®å¤ã„å†…å®¹ã¨æ–°ã—ã„å†…å®¹ã§ã™ã€‚ä½•ãŒå¤‰æ›´ã•ã‚ŒãŸã‹ã‚’ç°¡æ½”ã«æ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n"
        "---å¤ã„å†…å®¹---\n"
        f"{old_text[:3000]}\n"
        "---æ–°ã—ã„å†…å®¹---\n"
        f"{new_text[:3000]}"
    )
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å¤‰æ›´ç‚¹ã‚’è¦ç´„ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[è¦ç´„å¤±æ•—]: {e}"

# ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆæ¯æ—¥1å›ï¼‰
def check_full_site_update():
    print("ğŸŒ ã‚µã‚¤ãƒˆå…¨ä½“ã®å·¡å›ã‚’é–‹å§‹ã—ã¾ã™...")
    base_url = "https://sun-name.com/"
    new_content = crawl_all_pages(base_url)
    old_content = get_existing_links()

    if new_content.strip() != old_content.strip():
        diff_summary = summarize_diff(old_content, new_content)
        update_links_and_log_diff(new_content, diff_summary)
        print("âœ… å·®åˆ†ã‚ã‚Šï¼šæ›´æ–°ãƒ»è¨˜éŒ²ã—ã¾ã—ãŸ")
    else:
        print("å¤‰åŒ–ãªã—ï¼šæ›´æ–°ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# æ¯æ—¥åˆå‰3æ™‚ã«å®Ÿè¡Œï¼ˆå®Ÿé‹ç”¨ã§ã¯cronæ¨å¥¨ï¼‰
if __name__ == "__main__":
    while True:
        now = datetime.datetime.now()
        if now.hour == 3:
            check_full_site_update()
            time.sleep(24 * 60 * 60)  # 24æ™‚é–“å¾…æ©Ÿ
        else:
            time.sleep(60 * 30)  # 30åˆ†ã”ã¨ã«å†ç¢ºèª
