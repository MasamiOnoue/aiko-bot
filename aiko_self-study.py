# aiko_self_study.py

import requests
import hashlib
import time
import datetime
import os
import re
from bs4 import BeautifulSoup
from googleapiclient.discovery import build
from google.oauth2.service_account import Credentials
import openai
import threading

# OpenAI APIã‚­ãƒ¼ï¼ˆRenderã®Environmentã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ï¼‰
openai.api_key = os.getenv("OPENAI_API_KEY")

# Spreadsheet IDï¼ˆä¼šç¤¾æƒ…å ±ï¼‰
SPREADSHEET_ID4 = os.getenv('SPREADSHEET_ID4')  # ä¼šç¤¾æƒ…å ±

# ä½¿ç”¨ã™ã‚‹ã‚¹ã‚³ãƒ¼ãƒ—ã¨èªè¨¼æƒ…å ±
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]
creds = Credentials.from_service_account_file("service_account.json", scopes=SCOPES)
sheet_service = build("sheets", "v4", credentials=creds)

# ä¼šè©±ãƒ­ã‚°é–¢é€£ã®èª­ã¿è¾¼ã¿
from company_info import get_conversation_log, load_all_user_ids

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆå…¨ä½“ä¼šè©±ãƒ­ã‚°50ä»¶ã€å€‹åˆ¥ãƒ­ã‚°20ä»¶ï¼‰
user_conversation_cache = {}
full_conversation_cache = []

# ç‰¹å®šã®ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€é‡è¦ä¼šè©±ãƒ•ãƒ©ã‚°
IMPORTANT_PATTERNS = [
    "é‡è¦", "ç·Šæ€¥", "è‡³æ€¥", "è¦ç¢ºèª", "ãƒˆãƒ©ãƒ–ãƒ«", "å¯¾å¿œã—ã¦", "ã™ãã«", "å¤§è‡³æ€¥"
]

# é‡è¦ãªä¼šè©±ã‚’ä¿å­˜é–¢æ•°
def is_important_message(text):
    pattern = "|".join(map(re.escape, IMPORTANT_PATTERNS))
    return re.search(pattern, text, re.IGNORECASE) is not None

# é‡è¦ãªä¼šè©±ã‚’ä¿å­˜é–¢æ•°
def clean_log_message(text):
    patterns = [
        "è¦šãˆã¦ãã ã•ã„", "è¦šãˆã¦", "ãŠã¼ãˆã¦ãŠã„ã¦", "è¦šãˆã¦ã­",
        "è¨˜éŒ²ã—ã¦", "ãƒ¡ãƒ¢ã—ã¦", "å¿˜ã‚Œãªã„ã§", "è¨˜æ†¶ã—ã¦",
        "ä¿å­˜ã—ã¦", "è¨˜éŒ²ãŠé¡˜ã„", "è¨˜éŒ²ã‚’ãŠé¡˜ã„"
    ]
    pattern = "|".join(map(re.escape, patterns))
    return re.sub(pattern, "", text, flags=re.IGNORECASE).strip()

# é‡è¦ãªä¼šè©±ã‚’ä¿å­˜é–¢æ•°
def cache_all_user_conversations():
    logs = get_conversation_log()
    all_user_ids = load_all_user_ids()

    global full_conversation_cache
    full_conversation_cache = []
    
    # å…¨ä½“ã®æœ€æ–°50ä»¶ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    for log in logs[-100:]:
        if len(log) > 4:
            speaker = log[3]
            message = clean_log_message(log[4])
            flag = " [é‡è¦]" if is_important_message(message) else ""
            full_conversation_cache.append(f"{speaker}: {message}{flag}")

    # å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æœ€æ–°20ä»¶ã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    for user_id in all_user_ids:
        user_logs = [
            f"{log[3]}: {clean_log_message(log[4])}{' [é‡è¦]' if is_important_message(log[4]) else ''}"
            for log in logs if len(log) > 4 and log[1] == user_id
        ][-20:]
        user_conversation_cache[user_id] = "\n".join(user_logs)

    print("ğŸ§  ä¼šè©±ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°ã—ã¾ã—ãŸ")

# 10åˆ†ã”ã¨ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°
cache_thread = threading.Thread(target=lambda: periodic_cache_update(600), daemon=True)

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def periodic_cache_update(interval):
    while True:
        cache_all_user_conversations()
        time.sleep(interval)

# ç›´è¿‘ã®ä¼šè©±ã®ç²¾æŸ»
def generate_contextual_reply(user_id, user_message):
    user_context = user_conversation_cache.get(user_id, "")
    others_context = "\n".join(full_conversation_cache)
    prompt = (
        f"ä»¥ä¸‹ã¯ã“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ç›´è¿‘ã®ä¼šè©±ã¨ã€ç¤¾å†…ã§äº¤ã‚ã•ã‚ŒãŸä»–ã®ä¼šè©±ã®è¨˜éŒ²ã§ã™ã€‚æ–‡è„ˆã‚’è¸ã¾ãˆã¦ã€è‡ªç„¶ã«å¿œç­”ã—ã¦ãã ã•ã„ã€‚\n"
        f"ã€ã“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å±¥æ­´ã€‘\n{user_context}\n\n"
        f"ã€ä»–ã®äººã®è©±é¡Œã‚„ç¤¾å†…èƒŒæ™¯ã€‘\n{others_context}\n\n"
        f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›: {user_message}"
    )
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆæ„›å­ã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[å¿œç­”å¤±æ•—]: {e}"

# è£œè¶³æƒ…å ±åˆ—ã®å–å¾—ã¨æ›¸ãè¾¼ã¿
def get_existing_links():
    result = sheet_service.spreadsheets().values().get(
        spreadsheetId=SPREADSHEET_ID4,
        range="ä¼šç¤¾æƒ…å ±!F2"
    ).execute()
    values = result.get("values", [])
    return values[0][0] if values else ""

def update_links_and_log_diff(new_links_text, diff_summary):
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
    sheet_service.spreadsheets().values().update(
        spreadsheetId=SPREADSHEET_ID4,
        range="ä¼šç¤¾æƒ…å ±!F2",
        valueInputOption="USER_ENTERED",
        body={"values": [[new_links_text]]}
    ).execute()
    sheet_service.spreadsheets().values().append(
        spreadsheetId=SPREADSHEET_ID4,
        range="ä¼šç¤¾æƒ…å ±!G2",
        valueInputOption="USER_ENTERED",
        insertDataOption="INSERT_ROWS",
        body={"values": [[now, diff_summary]]}
    ).execute()

# ã‚µã‚¤ãƒˆå…¨ä½“ã‹ã‚‰ãƒªãƒ³ã‚¯ã¨ä¸­èº«ã‚’å–å¾—
def crawl_all_pages(base_url):
    try:
        response = requests.get(base_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        links = [a["href"] for a in soup.find_all("a", href=True) if base_url in a["href"]]
        unique_links = sorted(set(links))
        all_content = ""
        for link in unique_links:
            try:
                page = requests.get(link)
                page.raise_for_status()
                page_soup = BeautifulSoup(page.text, "html.parser")
                page_text = page_soup.get_text().strip()
                all_content += f"\n\n--- {link} ---\n{page_text}"
            except:
                continue
        return all_content
    except Exception as e:
        return f"[å·¡å›ã‚¨ãƒ©ãƒ¼]: {e}"

# OpenAIã§å·®åˆ†è¦ç´„
def summarize_diff(old_text, new_text):
    prompt = (
        "ä»¥ä¸‹ã¯Webãƒšãƒ¼ã‚¸ã®å¤ã„å†…å®¹ã¨æ–°ã—ã„å†…å®¹ã§ã™ã€‚ä½•ãŒå¤‰æ›´ã•ã‚ŒãŸã‹ã‚’ç°¡æ½”ã«æ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n"
        "---å¤ã„å†…å®¹---\n"
        f"{old_text[:3000]}\n"
        "---æ–°ã—ã„å†…å®¹---\n"
        f"{new_text[:3000]}"
    )
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å¤‰æ›´ç‚¹ã‚’è¦ç´„ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[è¦ç´„å¤±æ•—]: {e}"

# ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆæ¯æ—¥1å›ï¼‰
def check_full_site_update():
    print("ğŸŒ ã‚µã‚¤ãƒˆå…¨ä½“ã®å·¡å›ã‚’é–‹å§‹ã—ã¾ã™...")
    base_url = "https://sun-name.com/"
    new_content = crawl_all_pages(base_url)
    old_content = get_existing_links()

    if new_content.strip() != old_content.strip():
        diff_summary = summarize_diff(old_content, new_content)
        update_links_and_log_diff(new_content, diff_summary)
        print("âœ… å·®åˆ†ã‚ã‚Šï¼šæ›´æ–°ãƒ»è¨˜éŒ²ã—ã¾ã—ãŸ")
    else:
        print("å¤‰åŒ–ãªã—ï¼šæ›´æ–°ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# å®Ÿè¡Œé–‹å§‹
if __name__ == "__main__":
    cache_thread.start()
    while True:
        now = datetime.datetime.now()
        if now.hour == 3:
            check_full_site_update()
            time.sleep(24 * 60 * 60)
        else:
            time.sleep(60 * 30)
